# Introduction to Responsible AI

## Contexts
* [Course](#course)
* [TIL](#til)
* [Notes of the wrong answers](#notes-of-the-wrong-answers)

## Course
* [x] VIDEO: Introduction to Responsible AI
* [x] QUIZ: Introduction to Responsible AI: Quiz

## TIL
강의의 내용은 다음의 과정으로 전개됨.
* Understand why Google has put AI principles in place
* Identify the need for responsible AI practice within an organization
* Recognize that decisions made at all stages of a project have an impact on responsible AI
* Recognize the organizations can design AI to fit their own business needs and values

**common themes of responsible AI**
* Transparency
* Fairness
* Accountability
* Privacy

> Every decision point requires consideration and evaluation to ensure that choices have been made responsibly.

Concept(Design), Development, Deployment, Monitoring 과 같이 제품의 시작부터 끝까지 고려 사항 및 평가가 필요함을 강조하는 내용은 단순 Gen AI를 위한 것은 아니고, 원론적인 SW 개발에 대한 각 작업자들이 가져야 할 태도로 알고 있음.

그러나 유독 Gen AI에서 책임감 있는 AI 개발을 위한 내용을 강조하는 것은, Gen AI가 다양한 분야에서 활용될 수 있으며 그에 따라 크고 작은 영향을 미치는 것이 점차 치명적일 수 있기 때문에 보다 더 강조하는 듯함.

**책임감 있는 AI 개발**

a. 논란의 여지가 있는 AI 사용 사례를 식별하고, 이를 해결하기 위한 방법을 찾아야 함.

b. 책임감 있는 AI 관행을 필요로 함.

* AI 관행: AI를 개발하고 사용하는 데 필요한 모든 것을 포함하는 일련의 행동, 절차, 프로세스, 정책, 도구, 지침, 규정
* AI 관행 필요 이유:
    * 아래의 사례가 윤리적 문제나 의도치 않은 결과 야기
        * 위험하지 않게 보이는 AI 사용 사례
        * 선한 의도를 가진 사용 사례
    * 기대한 만큼 유익하지 않은 경우 방지

**7 AI Principles(2018.06, Google)**
1. Be socially beneficial
2. Avoid creating or reinforcing unfair bias(race, ethnicity, gender, nationality, income, sexual orientation, ability, and political or religious belief, etc.)
3. Be built and tested for safety
4. Be accountable to people
5. Incorporate privacy design principles
6. Uphold high standards of scientific excellence
7. Be made available for uses that accord with these principles

**Applications Google will not pursue**
- Technologies that cause or are likely to cause overall harm.
- Weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.
- Technologies that gather or use information for surveillance violating internationally accepted norms.
- Technologies whose purpose contravenes widely accepted principles of international law and human rights.

앞서 메모한 바와 같이, 연구 윤리 및 책임감 있는 개발에 대한 내용은 꽤 오래 전부터 과학자들 사이에서도 논의되어 왔(다고 들)음.

최근 알쓸별잡에서 놀란 감독과 오펜하이머에 대해 패널 토론을 하는 장면을 봤는데, 김상욱 교수가 의역한 오펜하이머의 인용문이 나옴.
> 과학자에겐 개발한 기술의 사용 여부를 결정할 권한이 없다. 그러나 과학자는 기술이 사용될 때 어떤 영향을 미칠지에 대해 고려해야 한다.

이후 오펜하이머(J. Robert Oppenheimer)란 과학자가 궁금해져 인용문을 찾아 보았는데, 본 강의를 보면서 아래의 인용문이 떠오름.
> Scientists are not delinquents. Our work has changed the conditions in which men live, but the use made of these changes is the problem of governments, not of scientists.

> Knowledge cannot be pursued without morality.

> The experience of seeing how our thought and our words and our ideas have been confined by the limitation of out experience is one which is salutary and is in a certain sense good for a man's morals as well as good for his pleasure. It seems to us \[scientists\] that this is an opening up of the human spirit, avoiding its provincialism and narrowness.

근래에 윤리 원칙을 두는 것은 윤리적 딜레마 상황에서 올바른 행동 방향을 제시하기 위함임. 그것을 따르는 것은 개인의 선택이고, 그것을 따르지 않는 것 또한 개인의 선택임. 그러나 윤리적 딜레마 상황에서 올바른 행동 방향을 제시하는 것은 과학자의 책임이라는 것을 알 수 있음.

인용문을 바탕으로 이러한 책임에 대해 더 기술하자면, 행동 방향 제시는 강제성이 없기 때문에 과학자가 책임감 있는 AI 개발을 위해 노력한다고 해서 모든 문제가 해결되는 것은 아님. AI를 사용하는 사람(혹은 정부, 이하 사람들) 또한 책임감 있게 사용해야 함. 만약 AI를 사용하는 사람들이 책임감 있게 사용하지 않는다면, AI 개발자의 노력이 무색해질 수 있음.

마지막으로 구글에서도 모든 지침 혹은 정책을 두어 모든 문제를 해결할 수 없다고 함. 모든 상황을 규범지으려는 것은 불가능하나, 명시되지 않은 상황에서도 개인의 판단에 따라 책임감 있게 행동하기 위해서는 인류의 공통적인 가치를 고려하되 도덕적인 판단을 통해 행동해야 함.

## Notes of the wrong answers
(none)
